{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done:\n",
    "* This code displays available compute devices and uses GPU if available\n",
    "* Defines scenes as strings of 0/1/2 meaning element in B-A/element in A\\&B/irrelevants\n",
    "* Built Quantifier class hierarchy that judges scenes and generates prototypical scenes\n",
    "* Teach AE given unsupervised train scenes\n",
    "* Prints out network structure png files and onto noteboo\n",
    "* Compared AE unsupervised reconstructions of train scenes and other quantifiers\n",
    "* Tried and was pleased to see that not permutating the scenes causes overfitting as expected\n",
    "***\n",
    "* Explored why sometimes the training explodes in terms of loss!\n",
    "    - Replacing 'relu' activation function with 'sigmoid'/'tanh' eliminates the problem\n",
    "    - Even 'leakyrelu' alleviates it somewhat, must be exploding gradients\n",
    "    - Reduced  parameter (layer) size, lr, optimizer which alleviated the problem also\n",
    "    - Read related blogs, Another option is gradient clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "<li> compare natural quantifiers to general quantifiers </li>\n",
    "<li> build supervised quantifier classifier on the hidden vectors (softmax output) </li>\n",
    "<li> compare classification of natural classifiers to non general quantifiers  </li>\n",
    "<li> run this on random world (general quantifiable sets) </li>\n",
    "<li> run this on unnatural quantifier world generated by unnatural quantifier scenes </li>\n",
    "<li> run this on natural quantifier world generated by natural quantifier scenes </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define quantifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "# define 3 symbols for, member in A and B member in b but not a and irrelevant item scenes\n",
    "b_a_symbol = 0  # elements in B but not in A (B-A)\n",
    "ab_symbol = 1  # elements in A and in B (A\\&B)\n",
    "dont_care_symbol = 2  # irrelevant elements\n",
    "\n",
    "symbols = [b_a_symbol,\n",
    "           ab_symbol,\n",
    "           dont_care_symbol]\n",
    "\n",
    "class Quantifier(ABCMeta):\n",
    "\n",
    "    scenes_len = 1000\n",
    "    \n",
    "    # scene generative methods\n",
    "    @classmethod\n",
    "    def generate(cls):\n",
    "        \"\"\"\n",
    "        A scene is generated by permutating a scene prototype which is an ordered list of element symbols\n",
    "        First consecutive 1 element symbols representing elements in A and B.\n",
    "        Second consecutive 0 element symbols representing elements in B but not in A.\n",
    "        Third consecutive 2 element symbols representing elements irrelevant to the qunatification.\n",
    "        \"\"\"\n",
    "        return np.random.permutation(cls.generate_scene_prototype())\n",
    "#         return cls.generate_scene_prototype()\n",
    "    \n",
    "    @classmethod\n",
    "    def generate_scene_prototype(cls):\n",
    "        \"\"\"\n",
    "        Generate a scene prototype, a scene prototype is an ordered list of element symbols\n",
    "        First consecutive 1 element symbols representing elements in A and B.\n",
    "        Second consecutive 0 element symbols representing elements in B but not in A.\n",
    "        Third consecutive 2 element symbols representing elements irrelevant to the qunatification.\n",
    "        \n",
    "        This could have been left as an abstract method but we have a way to generate a prototypical\n",
    "        scene without requiring the specific quantifier to define a scene generative method.\n",
    "        By default we generate a scene prototype by generating a random scene\n",
    "        till it passes the quantifier method.\n",
    "        \n",
    "        NOTE: You should refrain from using this default behavior in general\n",
    "        since generating a quantifier matching scene by random\n",
    "        might take an arbitrary long ammount of time\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            scene = np.random.choice(symbols, cls.scenes_len)\n",
    "            if cls.quantifier(scene):\n",
    "                return scene\n",
    "\n",
    "    @classmethod\n",
    "    def scene_prototype(cls, ab_len, b_a_len):\n",
    "        # make sure we received legal parameters and then generate the scene prototype\n",
    "        assert(ab_len >= 0 and b_a_len >= 0 and ab_len + b_a_len <= cls.scenes_len)\n",
    "        return np.array([ab_symbol] * ab_len + \n",
    "                        [b_a_symbol] * b_a_len + \n",
    "                        [dont_care_symbol] * (cls.scenes_len - (ab_len + b_a_len)))\n",
    "\n",
    "    # scene quantification methods\n",
    "    @classmethod\n",
    "    def quantify(cls, scene):\n",
    "        # call the quantifier with as that are also bs and bs that are not as\n",
    "        unique_counts = dict(zip(*np.unique(scene, return_counts=True)))\n",
    "        result = cls.quantification(unique_counts[ab_symbol] if ab_symbol in unique_counts else 0,\n",
    "                                    unique_counts[b_a_symbol] if b_a_symbol in unique_counts else 0)\n",
    "        return result\n",
    "    \n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def quantification(cls, ab, b_a):\n",
    "        pass\n",
    "\n",
    "# add attributes class decorator\n",
    "def add_attributes(attr_dict):\n",
    "    def decorated(cls):\n",
    "        for attr_name, attr_value in attr_dict.items():\n",
    "            setattr(cls, attr_name, attr_value)\n",
    "        return cls\n",
    "    return decorated\n",
    "\n",
    "# add n parameter to given quantifier\n",
    "def n_quantifier(n, quantifier):\n",
    "    @add_attributes({\"n\":n})\n",
    "    class NQuantifier(quantifier):\n",
    "        pass\n",
    "    return NQuantifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Quantifier definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most quantifier\n",
    "class Most(Quantifier):\n",
    "    @classmethod\n",
    "    def generate_scene_prototype(cls):\n",
    "        # We assume implicature, i.e. if there is more than one element in A&B 'most' excludes 'all', B-A is not empty\n",
    "        ab_len = np.random.randint(1, high=max(cls.scenes_len, 2))\n",
    "        b_a_len = 0 if ab_len == 1 else np.random.randint(1, min(ab_len, cls.scenes_len - ab_len + 1))\n",
    "        \n",
    "        return cls.scene_prototype(ab_len, b_a_len)\n",
    "\n",
    "    @classmethod\n",
    "    def quantification(cls, ab, b_a):\n",
    "        return ab > b_a\n",
    "\n",
    "# Some quantifier\n",
    "class Some(Quantifier):\n",
    "    @classmethod\n",
    "    def generate_scene_prototype(cls):\n",
    "        ab_len = np.random.randint(1, high=cls.scenes_len)\n",
    "        b_a_len = np.random.randint(0, high=cls.scenes_len - ab_len + 1)\n",
    "        return cls.scene_prototype(ab_len, b_a_len)\n",
    "\n",
    "    @classmethod\n",
    "    def quantification(cls, ab, _):\n",
    "        return ab > 0\n",
    "\n",
    "\n",
    "# No quantifier\n",
    "class No(Quantifier):\n",
    "    @classmethod\n",
    "    def generate_scene_prototype(cls):\n",
    "        b_a_len = np.random.randint(0, high=cls.scenes_len)\n",
    "        return cls.scene_prototype(0, b_a_len)\n",
    "\n",
    "    @classmethod\n",
    "    def quantification(cls, ab, _):\n",
    "        return ab == 0\n",
    "\n",
    "# Every quantifier\n",
    "class Every(Quantifier):\n",
    "    @classmethod\n",
    "    def generate_scene_prototype(cls):\n",
    "        ab_len = np.random.randint(1, high=cls.scenes_len + 1)\n",
    "        return cls.scene_prototype(ab_len, 0)\n",
    "\n",
    "    @classmethod\n",
    "    def quantification(cls, _, b_a):\n",
    "        return b_a == 0\n",
    "    \n",
    "# N quantifier (requires the n member to operate)\n",
    "class N(Quantifier):    \n",
    "    @classmethod\n",
    "    def generate_scene_prototype(cls):\n",
    "        ab_len = cls.n\n",
    "        b_a_len = np.random.randint(cls.n, high=cls.scenes_len - ab_len + 1)\n",
    "        return cls.scene_prototype(ab_len, b_a_len)\n",
    "\n",
    "    @classmethod\n",
    "    def quantification(cls, ab, _):\n",
    "        return ab == cls.n\n",
    "\n",
    "One = n_quantifier(1, N)\n",
    "Both = n_quantifier(2, N)\n",
    "Two = Both\n",
    "Three = n_quantifier(3, N)\n",
    "Ten = n_quantifier(10, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generated quantified scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes_num = 1000\n",
    "\n",
    "def generate_quantified_scenes(quantifier, scenes_len=Quantifier.scenes_len, scenes_num=scenes_num):\n",
    "    # define input scenes\n",
    "    scenes = [quantifier.generate() for _ in range(scenes_num)]\n",
    "    # sanity check\n",
    "    assert(np.all([quantifier.quantify(scene) for scene in scenes]))\n",
    "\n",
    "    # make scenes a matrix for training\n",
    "    scenes = np.concatenate(scenes, axis=0)\n",
    "    # reshape input into [samples, timesteps, features]\n",
    "    scenes = scenes.reshape((scenes_num, Quantifier.scenes_len, 1))\n",
    "    return scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = generate_quantified_scenes(Most)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.2.0\n"
     ]
    }
   ],
   "source": [
    "# keras imports\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Bidirectional, RepeatVector, TimeDistributed, Dropout, LeakyReLU\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogicalDevice(name='/device:CPU:0', device_type='CPU'),\n",
       " LogicalDevice(name='/device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " LogicalDevice(name='/device:XLA_GPU:0', device_type='XLA_GPU'),\n",
       " LogicalDevice(name='/device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from keras import backend as K\n",
    "# K.tensorflow_backend._get_available_gpus()\n",
    "tf.config.list_logical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import partial, update_wrapper\n",
    "\n",
    "# def wrapped_partial(func, *args, **kwargs):\n",
    "#         partial_func = partial(func, *args, **kwargs)\n",
    "#         update_wrapper(partial_func, func)\n",
    "#         return partial_func\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "with tf.device(\"/cpu:0\"):\n",
    "# with tf.device(\"/gpu:0\"):\n",
    "    model = Sequential()\n",
    "    \n",
    "#     model.add(Bidirectional(LSTM(100, activation='relu', input_shape=(Quantifier.scenes_len, 1))))\n",
    "#     model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Bidirectional(LSTM(15, activation='sigmoid', input_shape=(Quantifier.scenes_len, 1))))\n",
    "#     model.add(Bidirectional(LSTM(5, activation=wrapped_partial(tf.nn.leaky_relu, alpha=0.01), return_sequences=True)))\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(RepeatVector(Quantifier.scenes_len))\n",
    "\n",
    "#     model.add(Bidirectional(LSTM(100, activation='relu', return_sequences=True)))\n",
    "#     model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Bidirectional(LSTM(15, activation='sigmoid', return_sequences=True)))\n",
    "#     model.add(Bidirectional(LSTM(5, activation=wrapped_partial(tf.nn.leaky_relu, alpha=0.01)unk, return_sequences=True)))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.9126\n",
      "Epoch 2/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.7045\n",
      "Epoch 3/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.6472\n",
      "Epoch 4/1000\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.6119\n",
      "Epoch 5/1000\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.5858\n",
      "Epoch 6/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5631\n",
      "Epoch 7/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5550\n",
      "Epoch 8/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5470\n",
      "Epoch 9/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5363\n",
      "Epoch 10/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5301\n",
      "Epoch 11/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5225\n",
      "Epoch 12/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5257\n",
      "Epoch 13/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5226\n",
      "Epoch 14/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5142\n",
      "Epoch 15/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5139\n",
      "Epoch 16/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5113\n",
      "Epoch 17/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5114\n",
      "Epoch 18/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5050\n",
      "Epoch 19/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4995\n",
      "Epoch 20/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5000\n",
      "Epoch 21/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4944\n",
      "Epoch 22/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4914\n",
      "Epoch 23/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4814\n",
      "Epoch 24/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4763\n",
      "Epoch 25/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4635\n",
      "Epoch 26/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4600\n",
      "Epoch 27/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4484\n",
      "Epoch 28/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4324\n",
      "Epoch 29/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4192\n",
      "Epoch 30/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4174\n",
      "Epoch 31/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4139\n",
      "Epoch 32/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4049\n",
      "Epoch 33/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4008\n",
      "Epoch 34/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3969\n",
      "Epoch 35/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3936\n",
      "Epoch 36/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3924\n",
      "Epoch 37/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3904\n",
      "Epoch 38/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3853\n",
      "Epoch 39/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3839\n",
      "Epoch 40/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3816\n",
      "Epoch 41/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3785\n",
      "Epoch 42/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3778\n",
      "Epoch 43/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3780\n",
      "Epoch 44/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3752\n",
      "Epoch 45/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3731\n",
      "Epoch 46/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3733\n",
      "Epoch 47/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3714\n",
      "Epoch 48/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3689\n",
      "Epoch 49/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3674\n",
      "Epoch 50/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3684\n",
      "Epoch 51/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3634\n",
      "Epoch 52/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3677\n",
      "Epoch 53/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3622\n",
      "Epoch 54/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3628\n",
      "Epoch 55/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3614\n",
      "Epoch 56/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3584\n",
      "Epoch 57/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3581\n",
      "Epoch 58/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3600\n",
      "Epoch 59/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3591\n",
      "Epoch 60/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3566\n",
      "Epoch 61/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3574\n",
      "Epoch 62/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3564\n",
      "Epoch 63/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3549\n",
      "Epoch 64/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3537\n",
      "Epoch 65/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3544\n",
      "Epoch 66/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3540\n",
      "Epoch 67/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3530\n",
      "Epoch 68/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3520\n",
      "Epoch 69/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3506\n",
      "Epoch 70/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3515\n",
      "Epoch 71/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3511\n",
      "Epoch 72/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3514\n",
      "Epoch 73/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3493\n",
      "Epoch 74/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3488\n",
      "Epoch 75/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3493\n",
      "Epoch 76/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3493\n",
      "Epoch 77/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3468\n",
      "Epoch 78/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3481\n",
      "Epoch 79/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3477\n",
      "Epoch 80/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3477\n",
      "Epoch 81/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3478\n",
      "Epoch 82/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3473\n",
      "Epoch 83/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3463\n",
      "Epoch 84/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3458\n",
      "Epoch 85/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3459\n",
      "Epoch 86/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3460\n",
      "Epoch 87/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3457\n",
      "Epoch 88/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3446\n",
      "Epoch 89/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3463\n",
      "Epoch 90/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3450\n",
      "Epoch 91/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3452\n",
      "Epoch 92/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3439\n",
      "Epoch 93/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3454\n",
      "Epoch 94/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3439\n",
      "Epoch 95/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3449\n",
      "Epoch 96/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3442\n",
      "Epoch 97/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3423\n",
      "Epoch 98/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3440\n",
      "Epoch 99/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3434\n",
      "Epoch 100/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3446\n",
      "Epoch 101/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3437\n",
      "Epoch 102/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3433\n",
      "Epoch 103/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3420\n",
      "Epoch 104/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3434\n",
      "Epoch 105/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3424\n",
      "Epoch 106/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3425\n",
      "Epoch 107/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3414\n",
      "Epoch 108/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3429\n",
      "Epoch 109/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3414\n",
      "Epoch 110/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3407\n",
      "Epoch 111/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3424\n",
      "Epoch 112/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3417\n",
      "Epoch 113/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3418\n",
      "Epoch 114/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3409\n",
      "Epoch 115/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3401\n",
      "Epoch 116/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3415\n",
      "Epoch 117/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3407\n",
      "Epoch 118/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3405\n",
      "Epoch 119/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3401\n",
      "Epoch 120/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3393\n",
      "Epoch 121/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3391\n",
      "Epoch 122/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3398\n",
      "Epoch 123/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3404\n",
      "Epoch 124/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3390\n",
      "Epoch 125/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3395\n",
      "Epoch 126/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3411\n",
      "Epoch 127/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3390\n",
      "Epoch 128/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3386\n",
      "Epoch 129/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3391\n",
      "Epoch 130/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3388\n",
      "Epoch 131/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3380\n",
      "Epoch 132/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3382\n",
      "Epoch 133/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3397\n",
      "Epoch 134/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3385\n",
      "Epoch 135/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3378\n",
      "Epoch 136/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3380\n",
      "Epoch 137/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3373\n",
      "Epoch 138/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3374\n",
      "Epoch 139/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3375\n",
      "Epoch 140/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3372\n",
      "Epoch 141/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3370\n",
      "Epoch 142/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3368\n",
      "Epoch 143/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3373\n",
      "Epoch 144/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3360\n",
      "Epoch 145/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3378\n",
      "Epoch 146/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3362\n",
      "Epoch 147/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3377\n",
      "Epoch 148/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3368\n",
      "Epoch 149/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3372\n",
      "Epoch 150/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3375\n",
      "Epoch 151/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3371\n",
      "Epoch 152/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3368\n",
      "Epoch 153/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3367\n",
      "Epoch 154/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3361\n",
      "Epoch 155/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3367\n",
      "Epoch 156/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3371\n",
      "Epoch 157/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3362\n",
      "Epoch 158/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3362\n",
      "Epoch 159/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3361\n",
      "Epoch 160/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3366\n",
      "Epoch 161/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3362\n",
      "Epoch 162/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3353\n",
      "Epoch 163/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3361\n",
      "Epoch 164/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3357\n",
      "Epoch 165/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3347\n",
      "Epoch 166/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3354\n",
      "Epoch 167/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3355\n",
      "Epoch 168/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3351\n",
      "Epoch 169/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3358\n",
      "Epoch 170/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3354\n",
      "Epoch 171/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3355\n",
      "Epoch 172/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3355\n",
      "Epoch 173/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3352\n",
      "Epoch 174/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3349\n",
      "Epoch 175/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3354\n",
      "Epoch 176/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3360\n",
      "Epoch 177/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3349\n",
      "Epoch 178/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3348\n",
      "Epoch 179/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3356\n",
      "Epoch 180/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3352\n",
      "Epoch 181/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3354\n",
      "Epoch 182/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3351\n",
      "Epoch 183/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3347\n",
      "Epoch 184/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3351\n",
      "Epoch 185/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3351\n",
      "Epoch 186/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3354\n",
      "Epoch 187/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3349\n",
      "Epoch 188/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3350\n",
      "Epoch 189/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3349\n",
      "Epoch 190/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3353\n",
      "Epoch 191/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3343\n",
      "Epoch 192/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3345\n",
      "Epoch 193/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3354\n",
      "Epoch 194/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3349\n",
      "Epoch 195/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3342\n",
      "Epoch 196/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3345\n",
      "Epoch 197/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3350\n",
      "Epoch 198/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3340\n",
      "Epoch 199/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3340\n",
      "Epoch 200/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3340\n",
      "Epoch 201/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3357\n",
      "Epoch 202/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3342\n",
      "Epoch 203/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3346\n",
      "Epoch 204/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3341\n",
      "Epoch 205/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3341\n",
      "Epoch 206/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3338\n",
      "Epoch 207/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3339\n",
      "Epoch 208/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3337\n",
      "Epoch 209/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3346\n",
      "Epoch 210/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3339\n",
      "Epoch 211/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3349\n",
      "Epoch 212/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3346\n",
      "Epoch 213/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3347\n",
      "Epoch 214/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3337\n",
      "Epoch 215/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3338\n",
      "Epoch 216/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3333\n",
      "Epoch 217/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3339\n",
      "Epoch 218/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3335\n",
      "Epoch 219/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3336\n",
      "Epoch 220/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3336\n",
      "Epoch 221/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3336\n",
      "Epoch 222/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3335\n",
      "Epoch 223/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3338\n",
      "Epoch 224/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3343\n",
      "Epoch 225/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3343\n",
      "Epoch 226/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3344\n",
      "Epoch 227/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3332\n",
      "Epoch 228/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3334\n",
      "Epoch 229/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3340\n",
      "Epoch 230/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3333\n",
      "Epoch 231/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3335\n",
      "Epoch 232/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3331\n",
      "Epoch 233/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3328\n",
      "Epoch 234/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3346\n",
      "Epoch 235/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3336\n",
      "Epoch 236/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3331\n",
      "Epoch 237/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3342\n",
      "Epoch 238/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3336\n",
      "Epoch 239/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3334\n",
      "Epoch 240/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3334\n",
      "Epoch 241/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3335\n",
      "Epoch 242/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3334\n",
      "Epoch 243/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3341\n",
      "Epoch 244/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3331\n",
      "Epoch 245/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3332\n",
      "Epoch 246/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3324\n",
      "Epoch 247/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3332\n",
      "Epoch 248/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3331\n",
      "Epoch 249/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3335\n",
      "Epoch 250/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3331\n",
      "Epoch 251/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3328\n",
      "Epoch 252/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3328\n",
      "Epoch 253/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3326\n",
      "Epoch 254/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3332\n",
      "Epoch 255/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3334\n",
      "Epoch 256/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3338\n",
      "Epoch 257/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3334\n",
      "Epoch 258/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3322\n",
      "Epoch 259/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3326\n",
      "Epoch 260/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3330\n",
      "Epoch 261/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3327\n",
      "Epoch 262/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3324\n",
      "Epoch 263/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3332\n",
      "Epoch 264/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3328\n",
      "Epoch 265/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3326\n",
      "Epoch 266/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3330\n",
      "Epoch 267/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3330\n",
      "Epoch 268/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3323\n",
      "Epoch 269/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3327\n",
      "Epoch 270/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3336\n",
      "Epoch 271/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3324\n",
      "Epoch 272/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3333\n",
      "Epoch 273/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3330\n",
      "Epoch 274/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3337\n",
      "Epoch 275/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3319\n",
      "Epoch 276/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3323\n",
      "Epoch 277/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3341\n",
      "Epoch 278/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3329\n",
      "Epoch 279/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3325\n",
      "Epoch 280/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3328\n",
      "Epoch 281/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3331\n",
      "Epoch 282/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3328\n",
      "Epoch 283/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3330\n",
      "Epoch 284/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3329\n",
      "Epoch 285/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3323\n",
      "Epoch 286/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3326\n",
      "Epoch 287/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3326\n",
      "Epoch 288/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3323\n",
      "Epoch 289/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3323\n",
      "Epoch 290/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3321\n",
      "Epoch 291/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3324\n",
      "Epoch 292/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3330\n",
      "Epoch 293/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3329\n",
      "Epoch 294/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3324\n",
      "Epoch 295/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3329\n",
      "Epoch 296/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3323\n",
      "Epoch 297/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3323\n",
      "Epoch 298/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3334\n",
      "Epoch 299/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3321\n",
      "Epoch 300/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3329\n",
      "Epoch 301/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3327\n",
      "Epoch 302/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3328\n",
      "Epoch 303/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3329\n",
      "Epoch 304/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3326\n",
      "Epoch 305/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3318\n",
      "Epoch 306/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3325\n",
      "Epoch 307/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3327\n",
      "Epoch 308/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3324\n",
      "Epoch 309/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3318\n",
      "Epoch 310/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3326\n",
      "Epoch 311/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3326\n",
      "Epoch 312/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3325\n",
      "Epoch 313/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3322\n",
      "Epoch 314/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3320\n",
      "Epoch 315/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3320\n",
      "Epoch 316/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3330\n",
      "Epoch 317/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3322\n",
      "Epoch 318/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3319\n",
      "Epoch 319/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3326\n",
      "Epoch 320/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3324\n",
      "Epoch 321/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3318\n",
      "Epoch 322/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3321\n",
      "Epoch 323/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3324\n",
      "Epoch 324/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3317\n",
      "Epoch 325/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3323\n",
      "Epoch 326/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3320\n",
      "Epoch 327/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3324\n",
      "Epoch 328/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3320\n",
      "Epoch 329/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3324\n",
      "Epoch 330/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3318\n",
      "Epoch 331/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3324\n",
      "Epoch 332/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3321\n",
      "Epoch 333/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3318\n",
      "Epoch 334/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3321\n",
      "Epoch 335/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3316\n",
      "Epoch 336/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 337/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3319\n",
      "Epoch 338/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3319\n",
      "Epoch 339/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3308\n",
      "Epoch 340/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3312\n",
      "Epoch 341/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3329\n",
      "Epoch 342/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3317\n",
      "Epoch 343/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3318\n",
      "Epoch 344/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3318\n",
      "Epoch 345/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3311\n",
      "Epoch 346/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3314\n",
      "Epoch 347/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3314\n",
      "Epoch 348/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3324\n",
      "Epoch 349/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3314\n",
      "Epoch 350/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3316\n",
      "Epoch 351/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3318\n",
      "Epoch 352/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3321\n",
      "Epoch 353/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3323\n",
      "Epoch 354/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3316\n",
      "Epoch 355/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3318\n",
      "Epoch 356/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3315\n",
      "Epoch 357/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3316\n",
      "Epoch 358/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3321\n",
      "Epoch 359/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3323\n",
      "Epoch 360/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3324\n",
      "Epoch 361/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3325\n",
      "Epoch 362/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3316\n",
      "Epoch 363/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3317\n",
      "Epoch 364/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3324\n",
      "Epoch 365/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3318\n",
      "Epoch 366/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3315\n",
      "Epoch 367/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3316\n",
      "Epoch 368/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3319\n",
      "Epoch 369/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3316\n",
      "Epoch 370/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3315\n",
      "Epoch 371/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3320\n",
      "Epoch 372/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3317\n",
      "Epoch 373/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3323\n",
      "Epoch 374/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 375/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3312\n",
      "Epoch 376/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 377/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 378/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3310\n",
      "Epoch 379/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3322\n",
      "Epoch 380/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3326\n",
      "Epoch 381/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3321\n",
      "Epoch 382/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3316\n",
      "Epoch 383/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3312\n",
      "Epoch 384/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3316\n",
      "Epoch 385/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3317\n",
      "Epoch 386/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3314\n",
      "Epoch 387/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3320\n",
      "Epoch 388/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3311\n",
      "Epoch 389/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3308\n",
      "Epoch 390/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3311\n",
      "Epoch 391/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3325\n",
      "Epoch 392/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3326\n",
      "Epoch 393/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3310\n",
      "Epoch 394/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3317\n",
      "Epoch 395/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3320\n",
      "Epoch 396/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3318\n",
      "Epoch 397/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3312\n",
      "Epoch 398/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3319\n",
      "Epoch 399/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 400/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 401/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3316\n",
      "Epoch 402/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 403/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3314\n",
      "Epoch 404/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 405/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3316\n",
      "Epoch 406/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3314\n",
      "Epoch 407/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3316\n",
      "Epoch 408/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 409/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3312\n",
      "Epoch 410/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3315\n",
      "Epoch 411/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 412/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3316\n",
      "Epoch 413/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3311\n",
      "Epoch 414/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 415/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3319\n",
      "Epoch 416/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3312\n",
      "Epoch 417/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3317\n",
      "Epoch 418/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3322\n",
      "Epoch 419/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3315\n",
      "Epoch 420/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 421/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3316\n",
      "Epoch 422/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 423/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 424/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 425/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3308\n",
      "Epoch 426/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 427/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3310\n",
      "Epoch 428/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3319\n",
      "Epoch 429/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3311\n",
      "Epoch 430/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3319\n",
      "Epoch 431/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3310\n",
      "Epoch 432/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 433/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 434/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 435/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 436/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 437/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3311\n",
      "Epoch 438/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3314\n",
      "Epoch 439/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3310\n",
      "Epoch 440/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 441/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3311\n",
      "Epoch 442/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3310\n",
      "Epoch 443/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 444/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3311\n",
      "Epoch 445/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3317\n",
      "Epoch 446/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3312\n",
      "Epoch 447/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3315\n",
      "Epoch 448/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 449/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 450/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 451/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 452/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3312\n",
      "Epoch 453/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3315\n",
      "Epoch 454/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 455/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3311\n",
      "Epoch 456/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3315\n",
      "Epoch 457/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 458/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 459/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 460/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 461/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3310\n",
      "Epoch 462/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 463/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 464/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3311\n",
      "Epoch 465/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 466/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 467/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 468/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 469/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 470/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3310\n",
      "Epoch 471/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3315\n",
      "Epoch 472/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 473/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 474/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 475/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3310\n",
      "Epoch 476/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 477/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 478/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3310\n",
      "Epoch 479/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3311\n",
      "Epoch 480/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 481/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3308\n",
      "Epoch 482/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 483/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3308\n",
      "Epoch 484/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3316\n",
      "Epoch 485/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3317\n",
      "Epoch 486/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3318\n",
      "Epoch 487/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 488/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3318\n",
      "Epoch 489/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 490/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 491/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3308\n",
      "Epoch 492/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3310\n",
      "Epoch 493/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 494/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 495/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3310\n",
      "Epoch 496/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3312\n",
      "Epoch 497/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 498/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3312\n",
      "Epoch 499/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 500/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 501/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 502/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3308\n",
      "Epoch 503/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3311\n",
      "Epoch 504/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3314\n",
      "Epoch 505/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 506/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3311\n",
      "Epoch 507/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3308\n",
      "Epoch 508/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 509/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 510/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3311\n",
      "Epoch 511/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3311\n",
      "Epoch 512/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3312\n",
      "Epoch 513/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3315\n",
      "Epoch 514/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 515/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 516/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 517/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3310\n",
      "Epoch 518/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 519/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 520/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3310\n",
      "Epoch 521/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 522/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3308\n",
      "Epoch 523/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 524/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 525/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 526/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 527/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 528/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 529/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 530/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3314\n",
      "Epoch 531/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3310\n",
      "Epoch 532/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 533/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 534/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3310\n",
      "Epoch 535/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 536/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 537/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3311\n",
      "Epoch 538/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 539/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 540/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3314\n",
      "Epoch 541/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3308\n",
      "Epoch 542/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 543/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 544/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 545/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 546/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 547/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 548/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3311\n",
      "Epoch 549/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 550/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3311\n",
      "Epoch 551/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 552/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 553/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 554/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3299\n",
      "Epoch 555/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 556/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 557/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3310\n",
      "Epoch 558/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 559/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 560/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 561/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 562/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3308\n",
      "Epoch 563/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 564/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3314\n",
      "Epoch 565/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 566/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3312\n",
      "Epoch 567/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3299\n",
      "Epoch 568/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 569/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 570/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 571/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 572/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 573/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 574/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 575/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 576/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3311\n",
      "Epoch 577/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 578/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 579/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 580/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3296\n",
      "Epoch 581/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 582/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 583/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 584/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 585/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 586/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3312\n",
      "Epoch 587/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 588/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3310\n",
      "Epoch 589/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 590/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 591/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3308\n",
      "Epoch 592/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 593/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 594/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3313\n",
      "Epoch 595/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 596/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 597/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 598/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3310\n",
      "Epoch 599/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 600/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 601/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3310\n",
      "Epoch 602/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 603/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3314\n",
      "Epoch 604/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 605/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 606/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 607/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3308\n",
      "Epoch 608/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3308\n",
      "Epoch 609/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 610/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 611/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 612/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3311\n",
      "Epoch 613/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3308\n",
      "Epoch 614/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 615/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3298\n",
      "Epoch 616/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 617/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 618/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3315\n",
      "Epoch 619/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 620/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 621/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3299\n",
      "Epoch 622/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 623/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 624/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 625/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3299\n",
      "Epoch 626/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 627/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 628/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 629/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 630/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 631/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 632/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 633/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 634/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 635/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 636/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 637/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3311\n",
      "Epoch 638/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 639/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 640/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 641/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 642/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 643/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3312\n",
      "Epoch 644/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 645/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 646/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 647/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 648/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 649/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 650/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 651/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 652/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3311\n",
      "Epoch 653/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 654/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 655/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 656/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 657/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 658/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 659/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 660/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3308\n",
      "Epoch 661/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 662/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 663/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3296\n",
      "Epoch 664/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 665/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 666/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 667/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3296\n",
      "Epoch 668/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 669/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 670/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 671/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3294\n",
      "Epoch 672/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 673/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3308\n",
      "Epoch 674/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 675/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 676/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 677/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 678/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3309\n",
      "Epoch 679/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 680/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 681/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 682/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 683/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 684/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 685/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 686/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3298\n",
      "Epoch 687/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3298\n",
      "Epoch 688/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 689/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 690/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 691/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 692/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3294\n",
      "Epoch 693/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 694/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3296\n",
      "Epoch 695/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3312\n",
      "Epoch 696/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 697/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 698/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 699/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 700/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3296\n",
      "Epoch 701/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 702/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 703/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3298\n",
      "Epoch 704/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 705/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 706/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 707/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 708/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 709/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 710/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 711/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 712/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3310\n",
      "Epoch 713/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 714/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3308\n",
      "Epoch 715/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 716/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 717/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3299\n",
      "Epoch 718/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3310\n",
      "Epoch 719/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3296\n",
      "Epoch 720/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 721/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3298\n",
      "Epoch 722/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3299\n",
      "Epoch 723/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3298\n",
      "Epoch 724/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 725/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3298\n",
      "Epoch 726/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 727/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3296\n",
      "Epoch 728/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 729/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3299\n",
      "Epoch 730/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 731/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 732/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3299\n",
      "Epoch 733/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3299\n",
      "Epoch 734/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 735/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 736/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 737/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3298\n",
      "Epoch 738/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 739/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 740/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 741/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3308\n",
      "Epoch 742/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 743/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3295\n",
      "Epoch 744/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 745/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 746/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 747/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3296\n",
      "Epoch 748/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 749/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 750/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 751/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 752/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 753/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3298\n",
      "Epoch 754/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3294\n",
      "Epoch 755/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3294\n",
      "Epoch 756/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3299\n",
      "Epoch 757/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3299\n",
      "Epoch 758/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3295\n",
      "Epoch 759/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3296\n",
      "Epoch 760/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 761/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 762/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 763/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 764/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3294\n",
      "Epoch 765/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 766/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 767/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 768/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 769/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 770/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3299\n",
      "Epoch 771/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 772/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 773/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3298\n",
      "Epoch 774/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3298\n",
      "Epoch 775/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3295\n",
      "Epoch 776/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 777/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3291\n",
      "Epoch 778/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 779/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 780/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3299\n",
      "Epoch 781/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 782/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3293\n",
      "Epoch 783/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3296\n",
      "Epoch 784/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 785/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3293\n",
      "Epoch 786/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 787/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 788/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 789/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 790/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3310\n",
      "Epoch 791/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 792/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 793/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3295\n",
      "Epoch 794/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 795/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 796/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 797/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 798/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3293\n",
      "Epoch 799/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 800/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 801/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 802/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3296\n",
      "Epoch 803/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 804/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3296\n",
      "Epoch 805/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3294\n",
      "Epoch 806/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 807/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3293\n",
      "Epoch 808/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 809/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 810/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 811/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 812/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3308\n",
      "Epoch 813/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3299\n",
      "Epoch 814/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 815/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3298\n",
      "Epoch 816/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 817/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 818/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 819/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3296\n",
      "Epoch 820/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 821/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 822/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3296\n",
      "Epoch 823/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 824/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3299\n",
      "Epoch 825/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3303\n",
      "Epoch 826/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 827/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3299\n",
      "Epoch 828/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3299\n",
      "Epoch 829/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3298\n",
      "Epoch 830/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3296\n",
      "Epoch 831/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 832/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 833/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3298\n",
      "Epoch 834/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 835/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3295\n",
      "Epoch 836/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 837/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3302\n",
      "Epoch 838/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 839/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3298\n",
      "Epoch 840/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 841/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3299\n",
      "Epoch 842/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3293\n",
      "Epoch 843/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 844/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3292\n",
      "Epoch 845/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 846/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3294\n",
      "Epoch 847/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3292\n",
      "Epoch 848/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 849/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 850/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3298\n",
      "Epoch 851/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 852/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3296\n",
      "Epoch 853/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 854/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 855/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3298\n",
      "Epoch 856/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 857/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3307\n",
      "Epoch 858/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3295\n",
      "Epoch 859/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 860/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 861/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3300\n",
      "Epoch 862/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 863/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 864/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3298\n",
      "Epoch 865/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3296\n",
      "Epoch 866/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3293\n",
      "Epoch 867/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3296\n",
      "Epoch 868/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3295\n",
      "Epoch 869/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 870/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3295\n",
      "Epoch 871/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 872/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 873/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 874/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 875/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3298\n",
      "Epoch 876/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 877/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 878/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3299\n",
      "Epoch 879/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3296\n",
      "Epoch 880/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3299\n",
      "Epoch 881/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3296\n",
      "Epoch 882/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3305\n",
      "Epoch 883/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3306\n",
      "Epoch 884/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3301\n",
      "Epoch 885/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 886/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3299\n",
      "Epoch 887/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3294\n",
      "Epoch 888/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3298\n",
      "Epoch 889/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3296\n",
      "Epoch 890/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 891/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3299\n",
      "Epoch 892/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3304\n",
      "Epoch 893/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3295\n",
      "Epoch 894/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3294\n",
      "Epoch 895/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3294\n",
      "Epoch 896/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3297\n",
      "Epoch 897/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3296\n",
      "Epoch 898/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3296\n",
      "Epoch 899/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3295\n",
      "Epoch 900/1000\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3298\n",
      "Epoch 901/1000\n",
      " 384/1000 [==========>...................] - ETA: 1s - loss: 0.3181"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "# with tf.device(\"/gpu:0\"):\n",
    "    # fit model\n",
    "    num_epochs = 1000\n",
    "    model.fit(scenes, scenes, epochs=num_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plot_model(model, show_shapes=True, to_file='reconstruct_blstm_autoencoder.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5966423749923706, 0.9738353490829468, 1.0189926624298096, 0.7164274454116821, 0.3837203085422516, 0.7689893841743469, 0.566558301448822]\n",
      "[0.8291693329811096, 1.0087257623672485, 0.7083257436752319, 0.7692075371742249, 0.08321564644575119, 1.1723034381866455, 0.2613573372364044]\n",
      "[0.9181878566741943, 0.4394599497318268, 1.0348454713821411, 0.8170169591903687, 0.5182989239692688, 0.8696186542510986, 0.44712257385253906]\n",
      "[0.9716566205024719, 0.9182447195053101, 0.9982175230979919, 0.7529011368751526, 0.35258784890174866, 0.8643466234207153, 0.24981771409511566]\n",
      "[1.0122385025024414, 0.6092875599861145, 0.8769370913505554, 0.8734450340270996, 0.6093497276306152, 1.261580467224121, 0.27388009428977966]\n",
      "[1.0655733346939087, 1.0640660524368286, 0.8910741209983826, 0.6209003925323486, 0.6711185574531555, 1.036953091621399, 0.6580508351325989]\n",
      "[1.0284370183944702, 0.7892674803733826, 0.9454870820045471, 0.9565351009368896, 0.4813307821750641, 0.6274818181991577, 0.3067057728767395]\n",
      "[0.959048867225647, 0.9655265212059021, 1.1195731163024902, 0.655689001083374, 0.3940853476524353, 0.9758360981941223, 0.4181232154369354]\n",
      "[1.0160242319107056, 1.0340923070907593, 0.999123215675354, 0.7516040205955505, 0.32673731446266174, 1.078902244567871, 0.5040817856788635]\n",
      "[0.7165500521659851, 0.9468187689781189, 1.0433191061019897, 0.7325989007949829, 0.3755640387535095, 0.41670945286750793, 0.6209981441497803]\n",
      "[0.40638434886932373, 0.5373015403747559, 0.904744565486908, 0.9758937954902649, 0.21973752975463867, 0.7627437114715576, 0.10046372562646866]\n",
      "[0.6239137053489685, 0.7068269848823547, 0.7441360354423523, 0.7231810092926025, 0.5925940871238708, 0.987036943435669, 0.6538358330726624]\n",
      "[0.9109898209571838, 1.025139331817627, 0.7757858037948608, 0.9203386902809143, 0.5127720832824707, 1.1167912483215332, 0.2804926633834839]\n",
      "[0.6831917762756348, 0.8849186897277832, 0.8488785028457642, 0.7451027631759644, 0.37885287404060364, 0.991118311882019, 0.5654087662696838]\n",
      "[0.9967260956764221, 0.8436785936355591, 0.9701943397521973, 0.8433669805526733, 0.4830416142940521, 1.0852237939834595, 0.1522790491580963]\n",
      "[0.9233076572418213, 0.9802485108375549, 0.7878656983375549, 0.8744944930076599, 0.07848876714706421, 1.0453852415084839, 0.4131847023963928]\n",
      "[0.6664459705352783, 1.0600048303604126, 1.0024442672729492, 0.9546173810958862, 0.4681565463542938, 0.49603068828582764, 0.4847599267959595]\n",
      "[1.1113204956054688, 0.7633048295974731, 0.42691361904144287, 0.8304517865180969, 0.44090694189071655, 1.0766814947128296, 0.6043885946273804]\n",
      "[0.9671772718429565, 0.6892939805984497, 1.0112320184707642, 0.7735162973403931, 0.6669131517410278, 0.7904671430587769, 0.21575623750686646]\n",
      "[0.9949238300323486, 0.8319849967956543, 0.8294938802719116, 0.7497261166572571, 0.5512312650680542, 0.3224635720252991, 0.3737257122993469]\n",
      "[0.9847729206085205, 0.7493478655815125, 0.8613690733909607, 0.7770487070083618, 0.5461090207099915, 1.0527669191360474, 0.6024702787399292]\n",
      "[1.097165584564209, 0.8464113473892212, 0.9510906338691711, 0.7963546514511108, 0.9836310744285583, 0.534174382686615, 0.33288055658340454]\n",
      "[0.9812896847724915, 0.7266461849212646, 1.1534419059753418, 0.6876914501190186, 0.20738233625888824, 0.5533713698387146, 0.5051384568214417]\n",
      "[1.0112942457199097, 0.6881653070449829, 0.505692720413208, 0.7164680361747742, 0.4854589104652405, 0.4537445306777954, 0.5708678960800171]\n",
      "[0.3629581928253174, 0.8276529908180237, 0.9676889181137085, 0.8208440542221069, 0.7408906817436218, 0.9139488339424133, 0.19530582427978516]\n",
      "[0.9938924908638, 0.5891717672348022, 0.9012695550918579, 0.9584865570068359, 0.09281723946332932, 0.8320415616035461, 0.5097497701644897]\n",
      "[1.0074572563171387, 0.9599211812019348, 1.017306923866272, 0.8460448384284973, 0.18457932770252228, 0.9774596691131592, 0.247475266456604]\n",
      "[1.0059888362884521, 0.974723219871521, 0.7482849359512329, 0.6878646016120911, 0.3565719723701477, 0.5383843779563904, 0.13936513662338257]\n",
      "[0.9185499548912048, 1.0425629615783691, 0.8760833144187927, 0.7198156118392944, 0.4580369293689728, 0.6655071973800659, 0.6476715207099915]\n",
      "[0.449457585811615, 0.34793350100517273, 0.9889442324638367, 0.8981350064277649, 0.28278037905693054, 0.9503674507141113, 0.6444757580757141]\n",
      "[0.6290193200111389, 1.0659044981002808, 0.9175792932510376, 0.8233373761177063, 0.7024361491203308, 0.7331188917160034, 0.18899081647396088]\n",
      "[0.7094537615776062, 1.0601162910461426, 0.9840384125709534, 0.6963789463043213, 0.2309219241142273, 0.8215604424476624, 0.1859944760799408]\n",
      "[1.0192973613739014, 0.6308445930480957, 0.8572829961776733, 0.977664053440094, 0.41878020763397217, 0.5549187660217285, 0.37489214539527893]\n",
      "[0.9318352341651917, 0.9873316884040833, 0.7370559573173523, 0.873991072177887, 0.25841373205184937, 0.8552902936935425, 0.184481680393219]\n",
      "[1.0605359077453613, 0.9319231510162354, 0.7955735921859741, 0.9405661225318909, 0.7552823424339294, 1.157793402671814, 0.6459055542945862]\n",
      "[1.1078070402145386, 0.6720895171165466, 0.97281813621521, 0.8858677744865417, 0.45507127046585083, 1.0736562013626099, 0.5008026361465454]\n",
      "[0.9027013182640076, 0.7977226376533508, 0.7658333778381348, 0.772635281085968, 0.5708271265029907, 0.9008790850639343, 0.40124472975730896]\n",
      "[0.871399998664856, 0.9182689189910889, 0.45581722259521484, 0.5882918834686279, 0.27698877453804016, 0.9951480031013489, 0.5463116765022278]\n",
      "[1.0091867446899414, 0.6020756363868713, 0.985085129737854, 0.8748610615730286, 0.3891536295413971, 1.1582928895950317, 0.4940207600593567]\n",
      "[1.0274559259414673, 0.9417420029640198, 0.9533213973045349, 0.7511563301086426, 0.5661892294883728, 1.1495307683944702, 0.5952355265617371]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-931ffccd453e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mquantifier\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_quantifiers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtest_scenes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_quantified_scenes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscenes_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtest_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_scenes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_scenes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtest_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquantifier\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquantifier\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquantifier\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_quantifiers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/research/RESEARCH/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m         return training_arrays.test_loop(self, f, ins,\n\u001b[0m\u001b[1;32m   1358\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m                                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/research/RESEARCH/lib/python3.8/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/research/RESEARCH/lib/python3.8/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/research/RESEARCH/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/research/RESEARCH/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/research/RESEARCH/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/git/research/RESEARCH/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/research/RESEARCH/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "test_size = 1000\n",
    "test_quantifiers = [One, Two, Three, Ten, Some, No, Every, Most]\n",
    "test_results = { quantifier:[] for quantifier in test_quantifiers}\n",
    "for _ in range(test_size):\n",
    "    for quantifier in test_quantifiers:\n",
    "        test_scenes = generate_quantified_scenes(quantifier, scenes_num=1)\n",
    "        test_result = model.evaluate(test_scenes, test_scenes, verbose=0)\n",
    "        test_results[quantifier].append(test_result)\n",
    "    print([test_results[quantifier][-1] for quantifier in test_quantifiers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for quantifier in test_quantifiers:\n",
    "    print(quantifier.__name__ + (('(' + str(quantifier.n) + ')') if 'n' in quantifier.__dict__ else '') + \":\" +\n",
    "          str({f.__name__:f(test_results[quantifier]) for f in [np.min, np.max, np.mean, np.median]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 3.51195300e+10,  8.80654254e+10,  8.96772260e+10,  2.71741092e+10,\n",
       "          1.17616742e+10,  3.72559923e+09,  2.03102028e+11,  7.31270349e+10,\n",
       "          2.68637585e+11,  8.99188244e+10,  5.38577715e+10,  2.30930063e+10,\n",
       "          2.68621988e+11,  1.06358776e+11,  6.05600276e+10,  1.10462669e+10,\n",
       "          2.59395076e+11,  9.37140388e+10,  3.04444846e+10,  5.29528832e+09,\n",
       "          2.23910150e+11,  8.23882629e+10, -1.56729262e+10,  2.57164145e+10,\n",
       "         -2.49317949e+10,  1.68270987e+11,  2.71396700e+10,  1.23880129e+11,\n",
       "          5.05991045e+10,  1.70978083e+11,  7.28143299e+10,  1.12444785e+11,\n",
       "          3.64919849e+10,  9.55615642e+10,  4.24525046e+10,  8.50258821e+10,\n",
       "          7.18343117e+09,  1.06928030e+11,  2.87330509e+10,  1.32392026e+11,\n",
       "          3.93439150e+10,  6.91191316e+10, -3.71371546e+09,  8.72103772e+10,\n",
       "         -1.76270377e+10,  5.24159959e+10, -4.20698235e+10,  2.08593838e+10,\n",
       "         -6.25013064e+10, -9.44970138e+09, -7.11536968e+10, -3.14037596e+10,\n",
       "         -9.23811267e+10, -5.99336346e+10, -1.26891123e+11, -1.07796406e+11,\n",
       "         -1.49111161e+11, -1.29017004e+11, -1.64222321e+11, -6.83987804e+10,\n",
       "          6.27861504e+10,  3.87431096e+10,  2.12533023e+10, -1.40001903e+11,\n",
       "          5.90859674e+09, -2.49146032e+11,  2.17635127e+11, -9.97820744e+10,\n",
       "          6.45365514e+10,  1.99191183e+10,  4.93728932e+10,  3.92031724e+10,\n",
       "          2.88315904e+10,  4.70276588e+10,  5.22689044e+10,  7.27946609e+10,\n",
       "          5.45015235e+10,  9.97719900e+10,  9.10089503e+10,  1.50002303e+11,\n",
       "          3.10388122e+09,  3.19515750e+10,  1.98760428e+10,  3.07197747e+10,\n",
       "          2.52246938e+10,  3.51159706e+10,  3.08138107e+10,  3.69960387e+10,\n",
       "          4.03628114e+10,  4.03427942e+10,  3.35232000e+10,  3.05073869e+10,\n",
       "         -2.39557140e+10,  3.24582830e+10, -5.15695493e+10,  1.03826483e+10,\n",
       "         -7.13465119e+10,  2.12235080e+10, -6.03855749e+10,  2.40249078e+10],\n",
       "        dtype=float32),\n",
       "  array([2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 2, 1,\n",
       "         2, 2, 2, 2, 1, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 1, 2,\n",
       "         2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1,\n",
       "         1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 0, 2, 1, 2, 2, 2,\n",
       "         0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2])),\n",
       " (array([ 7.4266972e+09,  1.7947120e+10,  1.8273214e+10,  5.5358669e+09,\n",
       "          2.3931628e+09,  7.4863309e+08,  4.1412211e+10,  1.4901348e+10,\n",
       "          5.4768034e+10,  1.8323915e+10,  1.0976685e+10,  4.7054582e+09,\n",
       "          5.4764384e+10,  2.1675565e+10,  1.2342688e+10,  2.2496225e+09,\n",
       "          5.2884435e+10,  1.9099005e+10,  6.2096061e+09,  1.0801019e+09,\n",
       "          4.5649056e+10,  1.6791118e+10, -3.1964273e+09,  5.2386724e+09,\n",
       "         -5.0911165e+09,  3.4308665e+10,  5.5242639e+09,  2.5255064e+10,\n",
       "          1.0308453e+10,  3.4853716e+10,  1.4836154e+10,  2.2928398e+10,\n",
       "          7.4355497e+09,  1.9489673e+10,  8.6540493e+09,  1.7341295e+10,\n",
       "          1.4692022e+09,  2.1803655e+10,  5.8609157e+09,  2.7001731e+10,\n",
       "          8.0300073e+09,  1.4099505e+10, -7.4818995e+08,  1.7794032e+10,\n",
       "         -3.5806316e+09,  1.0703322e+10, -8.5582981e+09,  4.2737894e+09,\n",
       "         -1.2718747e+10, -1.9015995e+09, -1.4479014e+10, -6.3731517e+09,\n",
       "         -1.8800746e+10, -1.2185268e+10, -2.5828927e+10, -2.1939016e+10,\n",
       "         -3.0357387e+10, -2.6260144e+10, -3.3424855e+10, -1.3901977e+10,\n",
       "          1.2823807e+10,  7.9224745e+09,  4.3622636e+09, -2.8501258e+10,\n",
       "          1.2497188e+09, -5.0759401e+10,  4.4387365e+10, -2.0330637e+10,\n",
       "          1.3156012e+10,  3.8864115e+09,  1.0064272e+10,  7.9860070e+09,\n",
       "          5.8717153e+09,  9.5729848e+09,  1.0649358e+10,  1.4825338e+10,\n",
       "          1.1091423e+10,  2.0330252e+10,  1.8545412e+10,  3.0568120e+10,\n",
       "          6.1861658e+08,  6.5235630e+09,  4.0618829e+09,  6.2698173e+09,\n",
       "          5.1509233e+09,  7.1672668e+09,  6.2919506e+09,  7.5524925e+09,\n",
       "          8.2416819e+09,  8.2372378e+09,  7.3249536e+09,  6.5625841e+09,\n",
       "         -4.8908093e+09,  6.6394501e+09, -1.0557223e+10,  2.1175444e+09,\n",
       "         -1.4612042e+10,  4.2193764e+09, -1.1968948e+10,  5.4410071e+09],\n",
       "        dtype=float32),\n",
       "  array([2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2,\n",
       "         1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2,\n",
       "         1, 1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2,\n",
       "         2, 2, 1, 1, 2, 1, 2, 0, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2,\n",
       "         1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2])),\n",
       " (array([ 5.5586365e+09,  1.3938582e+10,  1.4192256e+10,  4.2997181e+09,\n",
       "          1.8591132e+09,  5.8097869e+08,  3.2160473e+10,  1.1573094e+10,\n",
       "          4.2533196e+10,  1.4231248e+10,  8.5249449e+09,  3.6544394e+09,\n",
       "          4.2530316e+10,  1.6834017e+10,  9.5858555e+09,  1.7471880e+09,\n",
       "          4.1069949e+10,  1.4832670e+10,  4.8217815e+09,  8.3862554e+08,\n",
       "          3.5451621e+10,  1.3040413e+10, -2.4821673e+09,  4.0696589e+09,\n",
       "         -3.9541315e+09,  2.6644404e+10,  4.2918129e+09,  1.9613979e+10,\n",
       "          8.0069115e+09,  2.7068338e+10,  1.1523639e+10,  1.7805976e+10,\n",
       "          5.7753431e+09,  1.5134696e+10,  6.7210885e+09,  1.3466343e+10,\n",
       "          1.1398666e+09,  1.6932255e+10,  4.5517061e+09,  2.0968624e+10,\n",
       "          6.2359040e+09,  1.0948606e+10, -5.8149939e+08,  1.3817548e+10,\n",
       "         -2.7815416e+09,  8.3102833e+09, -6.6480640e+09,  3.3162883e+09,\n",
       "         -9.8798961e+09, -1.4802419e+09, -1.1247708e+10, -4.9537894e+09,\n",
       "         -1.4604891e+10, -9.4682112e+09, -2.0064080e+10, -1.7044247e+10,\n",
       "         -2.3581745e+10, -2.0401113e+10, -2.5966782e+10, -1.0804711e+10,\n",
       "          9.9525908e+09,  6.1475482e+09,  3.3804526e+09, -2.2139838e+10,\n",
       "          9.6051866e+08, -3.9422206e+10,  3.4464477e+10, -1.5788236e+10,\n",
       "          1.0216459e+10,  3.0191404e+09,  7.8156370e+09,  6.2030085e+09,\n",
       "          4.5609411e+09,  7.4368983e+09,  8.2708024e+09,  1.1515513e+10,\n",
       "          8.6171218e+09,  1.5788717e+10,  1.4402397e+10,  2.3738921e+10,\n",
       "          4.8383206e+08,  5.0636692e+09,  3.1519539e+09,  4.8670802e+09,\n",
       "          3.9978186e+09,  5.5636362e+09,  4.8834734e+09,  5.8623119e+09,\n",
       "          6.3967683e+09,  6.3934612e+09,  5.6863734e+09,  5.0945649e+09,\n",
       "         -3.7899379e+09,  5.1523195e+09, -8.1856666e+09,  1.6441281e+09,\n",
       "         -1.1331461e+10,  3.3601393e+09, -9.5953818e+09,  3.8043244e+09],\n",
       "        dtype=float32),\n",
       "  array([2, 0, 2, 2, 1, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2,\n",
       "         1, 1, 1, 1, 0, 1, 1, 2, 2, 2, 0, 2, 2, 2, 1, 2, 2, 1, 2, 0, 2, 1,\n",
       "         0, 0, 2, 2, 0, 1, 2, 2, 1, 2, 1, 1, 2, 1, 0, 0, 1, 0, 2, 0, 2, 1,\n",
       "         2, 2, 1, 2, 1, 1, 0, 2, 2, 1, 0, 2, 0, 1, 1, 0, 2, 2, 1, 1, 2, 0,\n",
       "         1, 0, 2, 2, 1, 2, 0, 2, 0, 1, 2, 2])),\n",
       " (array([0.50694305, 0.6288722 , 0.6867618 , 0.7112361 , 0.7188233 ,\n",
       "         0.7184483 , 0.71459717, 0.70946556, 0.70410115, 0.6989871 ,\n",
       "         0.69431806, 0.6901401 , 0.6864409 , 0.6831835 , 0.6803238 ,\n",
       "         0.67781734, 0.6756223 , 0.6737012 , 0.6720205 , 0.6705503 ,\n",
       "         0.66926074, 0.66812897, 0.6671371 , 0.6662745 , 0.665532  ,\n",
       "         0.66489065, 0.66433525, 0.6638532 , 0.663434  , 0.66306895,\n",
       "         0.6627506 , 0.66247267, 0.66222984, 0.6620176 , 0.6618318 ,\n",
       "         0.6616693 , 0.6615269 , 0.66140234, 0.66129315, 0.66119766,\n",
       "         0.6611138 , 0.6610404 , 0.6609762 , 0.6609197 , 0.6608704 ,\n",
       "         0.6608272 , 0.66078955, 0.66075665, 0.6607282 , 0.66070366,\n",
       "         0.6606827 , 0.66066444, 0.6606491 , 0.66063607, 0.6606252 ,\n",
       "         0.66061616, 0.6606089 , 0.66060334, 0.6605994 , 0.66059685,\n",
       "         0.66059595, 0.66059655, 0.6605987 , 0.6606025 , 0.6606083 ,\n",
       "         0.66061604, 0.66062605, 0.6606385 , 0.6606541 , 0.6606731 ,\n",
       "         0.6606962 , 0.66072416, 0.660758  , 0.66079897, 0.6608485 ,\n",
       "         0.6609089 , 0.6609826 , 0.66107285, 0.6611844 , 0.6613222 ,\n",
       "         0.66149384, 0.66170865, 0.6619785 , 0.66231865, 0.6627482 ,\n",
       "         0.66329086, 0.66397434, 0.66483027, 0.6658905 , 0.6671805 ,\n",
       "         0.6687049 , 0.6704208 , 0.6721904 , 0.6737275 , 0.6742942 ,\n",
       "         0.67242223, 0.66517085, 0.64670205, 0.6055108 , 0.51936024],\n",
       "        dtype=float32),\n",
       "  array([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])),\n",
       " (array([ 1.08243460e+07,  1.49475550e+07,  2.43804860e+07,  2.98564608e+08,\n",
       "          3.20461640e+07,  2.77614464e+08,  2.41535600e+07,  2.20102848e+08,\n",
       "          4.14227320e+07,  2.25837872e+08,  9.11680880e+07,  3.64588448e+08,\n",
       "          1.64110048e+08,  8.56100560e+07,  1.63519800e+07, -7.59158050e+06,\n",
       "         -5.98835800e+06, -1.51193620e+07, -5.68961680e+07, -6.01713200e+07,\n",
       "         -6.09103960e+07, -9.70925200e+06, -2.53389760e+07,  2.08461424e+08,\n",
       "          5.61972000e+07,  1.68726896e+08,  7.82374080e+07,  1.59634016e+08,\n",
       "          4.91994520e+07,  1.44986320e+08,  5.14411360e+07,  1.21569864e+08,\n",
       "          4.44279080e+07,  1.12559352e+08,  4.05027240e+07,  9.43595280e+07,\n",
       "          2.58319440e+07,  7.12992880e+07,  7.96568400e+06,  8.58770800e+07,\n",
       "          2.50280740e+07,  1.36660896e+08,  6.47888050e+06,  9.26949360e+07,\n",
       "         -1.78123920e+07,  5.77989760e+07, -3.87916720e+07,  2.54751320e+07,\n",
       "         -5.91401160e+07, -5.57564300e+06, -7.33859840e+07, -3.31869320e+07,\n",
       "         -9.43955200e+07, -8.10720480e+07, -1.25250368e+08, -1.34299904e+08,\n",
       "         -1.46417872e+08, -2.01396704e+08, -7.88123200e+07, -1.33401112e+08,\n",
       "          7.09850200e+06,  3.77916520e+07, -1.18572632e+08,  1.69868480e+07,\n",
       "         -2.07463888e+08,  1.98878896e+08, -6.63688200e+07,  4.22599200e+07,\n",
       "          3.51264200e+07,  5.24730160e+07,  5.83309680e+07,  5.49238720e+07,\n",
       "          3.44293280e+07,  5.47370280e+07,  7.07217440e+07,  9.03771840e+07,\n",
       "          8.26580000e+07,  1.00015760e+08,  1.00027696e+08,  1.16465536e+08,\n",
       "          1.25403664e+08,  2.03518128e+08,  1.79128840e+07,  2.99497780e+07,\n",
       "          3.97226000e+07, -1.32369340e+07,  4.15286080e+07,  4.07196000e+07,\n",
       "          2.91478400e+07,  3.58703400e+07,  4.13964480e+07,  4.16489160e+07,\n",
       "          3.30786400e+07,  3.65048640e+07,  3.23697000e+07,  3.63617760e+07,\n",
       "          1.63579360e+07, -5.35220720e+07,  3.74957680e+07, -4.39888160e+07],\n",
       "        dtype=float32),\n",
       "  array([0, 2, 1, 2, 2, 2, 2, 1, 0, 0, 1, 2, 2, 2, 2, 1, 1, 0, 2, 0, 2, 1,\n",
       "         1, 2, 2, 2, 0, 2, 1, 2, 2, 0, 2, 1, 1, 0, 0, 2, 2, 2, 2, 0, 1, 1,\n",
       "         2, 2, 1, 2, 1, 2, 2, 2, 0, 0, 2, 2, 0, 2, 2, 1, 0, 2, 2, 1, 2, 2,\n",
       "         1, 2, 0, 1, 0, 1, 2, 2, 0, 2, 2, 0, 1, 2, 1, 1, 2, 0, 0, 1, 2, 2,\n",
       "         2, 2, 2, 2, 0, 1, 0, 1, 0, 2, 2, 2])),\n",
       " (array([0.34846228, 0.43440193, 0.4819225 , 0.50636613, 0.5177184 ,\n",
       "         0.52188206, 0.5222432 , 0.5207132 , 0.518354  , 0.515754  ,\n",
       "         0.5131679 , 0.5107494 , 0.5085847 , 0.50669444, 0.50506586,\n",
       "         0.5036529 , 0.5024392 , 0.50140315, 0.5005216 , 0.49977326,\n",
       "         0.49913877, 0.49860096, 0.49814522, 0.49775887, 0.49743134,\n",
       "         0.49715364, 0.4969179 , 0.49671787, 0.496548  , 0.49640363,\n",
       "         0.4962811 , 0.49617684, 0.4960882 , 0.49601287, 0.4959488 ,\n",
       "         0.4958942 , 0.49584782, 0.4958083 , 0.4957747 , 0.49574614,\n",
       "         0.49572176, 0.49570107, 0.49568343, 0.49566847, 0.49565572,\n",
       "         0.49564493, 0.49563575, 0.49562794, 0.49562132, 0.49561578,\n",
       "         0.49561113, 0.4956072 , 0.49560386, 0.49560124, 0.4955991 ,\n",
       "         0.4955973 , 0.495596  , 0.49559504, 0.49559438, 0.49559402,\n",
       "         0.49559408, 0.49559444, 0.49559504, 0.495596  , 0.49559736,\n",
       "         0.4955991 , 0.4956013 , 0.4956041 , 0.49560755, 0.49561185,\n",
       "         0.4956172 , 0.4956239 , 0.49563247, 0.49564332, 0.49565744,\n",
       "         0.49567568, 0.49569952, 0.4957307 , 0.49577177, 0.4958256 ,\n",
       "         0.49589562, 0.49598652, 0.4961031 , 0.4962507 , 0.49643362,\n",
       "         0.49665457, 0.4969114 , 0.49719316, 0.4974724 , 0.49769455,\n",
       "         0.4977597 , 0.49749506, 0.49663776, 0.49476087, 0.49113005,\n",
       "         0.48455447, 0.47286397, 0.45233387, 0.41661304, 0.35442805],\n",
       "        dtype=float32),\n",
       "  array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])),\n",
       " (array([ 3.76275337e+11,  9.43741010e+11,  9.61116307e+11,  2.91212100e+11,\n",
       "          1.26023287e+11,  3.98962852e+10,  2.17573360e+12,  7.83635841e+11,\n",
       "          2.87807137e+12,  9.63512697e+11,  5.76915898e+11,  2.47220896e+11,\n",
       "          2.87778721e+12,  1.13953302e+12,  6.48597275e+11,  1.18326641e+11,\n",
       "          2.77898697e+12,  1.00386413e+12,  3.25736006e+11,  5.65487903e+10,\n",
       "          2.38983237e+12,  8.81613603e+11, -1.75597060e+11,  2.78078358e+11,\n",
       "         -2.73160126e+11,  1.79049307e+12,  3.62839409e+11,  1.32623499e+12,\n",
       "          5.20478097e+11,  1.80797217e+12,  7.80121670e+11,  1.16496807e+12,\n",
       "          3.53056883e+11,  9.98731481e+11,  3.71256361e+11,  7.68230359e+11,\n",
       "         -6.90004541e+10,  9.99735230e+11,  1.71015111e+11,  1.22032253e+12,\n",
       "          2.37169328e+11,  5.57073236e+11, -1.46681135e+11,  9.25819798e+11,\n",
       "         -1.69343386e+11,  5.90783906e+11, -3.98763786e+11,  2.40721199e+11,\n",
       "         -5.80639719e+11, -1.98480159e+10, -8.27978809e+11, -3.86896822e+11,\n",
       "         -1.06645973e+12, -6.78315229e+11, -1.23549280e+12, -1.05157683e+12,\n",
       "         -1.63897501e+12, -1.44067969e+11,  1.05824502e+10, -1.65194865e+12,\n",
       "          4.48391021e+11, -1.67346674e+12,  2.11407195e+12, -4.43935850e+11,\n",
       "          1.09568924e+12,  7.10233096e+11,  7.00405252e+11,  4.59807523e+11,\n",
       "          7.11200342e+11,  4.14429741e+11,  3.56218962e+11,  3.13057313e+11,\n",
       "          1.27726313e+11,  4.68460569e+11,  3.61068659e+11,  6.15872070e+11,\n",
       "          4.31544795e+11,  9.45275273e+11, -1.67971832e+11,  9.34728008e+10,\n",
       "          5.37844711e+11,  1.17220495e+12, -3.58620201e+11,  1.57182198e+11,\n",
       "          5.54132767e+11,  8.66624340e+11,  1.57263200e+12, -1.27183831e+11,\n",
       "          4.29301432e+10,  7.76456634e+11,  1.84547069e+12, -7.88840448e+11,\n",
       "          2.04082184e+11,  7.29324650e+11,  2.46538856e+12, -7.99343116e+11,\n",
       "         -2.10265555e+11,  6.95992910e+11,  2.83883445e+12, -1.42522253e+11],\n",
       "        dtype=float32),\n",
       "  array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n",
       "         2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2])),\n",
       " (array([ 25431.232 ,  27541.22  ,  24490.314 ,  26712.576 ,  53435.11  ,\n",
       "           7246.4297,  39348.61  ,  47788.723 ,  24116.512 ,  56909.37  ,\n",
       "          25984.457 , 231742.27  ,  45686.113 , 261264.44  ,  39776.55  ,\n",
       "          61096.63  ,  13304.015 ,  56449.57  ,  24864.867 , 158839.62  ,\n",
       "          18808.744 , 224517.4   ,  24234.326 ,  13616.661 ,  19149.678 ,\n",
       "          11958.294 ,  11085.278 ,  10334.687 ,  14462.532 ,  12028.19  ,\n",
       "         187976.84  ,  98563.6   ,  36309.508 ,  22446.914 ,  14029.406 ,\n",
       "          12746.919 ,   9036.957 , -11409.818 , -30030.195 , -16122.8545,\n",
       "         -25045.078 , -13282.019 ,   1045.8159,   3641.5938,   5861.3315,\n",
       "           6444.6475,  11117.822 ,  10026.213 ,  13656.87  ,  17119.53  ,\n",
       "          11828.536 ,  28727.107 ,  -2515.8262, 115720.85  ,  60639.76  ,\n",
       "         -51460.594 ,  90112.695 , 171215.06  , 152738.25  , 136525.5   ,\n",
       "         144673.84  , 123734.305 , 123616.945 ,  64030.832 , 102649.36  ,\n",
       "          90281.49  ,  63181.418 ,  52590.95  ,  83186.65  ,  55605.504 ,\n",
       "          72067.16  ,  51213.645 ,  33008.27  ,  42155.332 ,  14325.243 ,\n",
       "          41438.223 ,  11070.518 ,  36638.848 ,  16608.512 ,  35911.332 ,\n",
       "          12833.28  ,  32324.05  ,  23451.99  ,  25470.832 ,  19579.525 ,\n",
       "          31134.979 ,  15238.36  ,  26142.117 ,  22427.982 ,  15189.214 ,\n",
       "          19879.836 ,  22104.475 ,  21794.654 ,  23218.266 ,  24674.615 ,\n",
       "          14087.125 ,   6113.708 ,  20347.51  ,  13247.968 ,  10766.331 ],\n",
       "        dtype=float32),\n",
       "  array([1, 1, 0, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         2, 1, 0, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 0,\n",
       "         1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 0, 0, 1, 2, 2, 1,\n",
       "         1, 2, 2, 2, 1, 2, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 1,\n",
       "         1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 2, 1])),\n",
       " (array([ 2.38805795e+11,  5.77302364e+11,  5.87823579e+11,  1.77980342e+11,\n",
       "          7.67279596e+10,  2.44866908e+10,  1.33211514e+12,  4.79237472e+11,\n",
       "          1.76179551e+12,  5.89267993e+11,  3.52826950e+11,  1.50980542e+11,\n",
       "          1.76149758e+12,  6.96942002e+11,  3.96592546e+11,  7.22601574e+10,\n",
       "          1.70113119e+12,  6.13807555e+11,  1.99210844e+11,  3.44771052e+10,\n",
       "          1.46311112e+12,  5.39192328e+11, -1.07517673e+11,  1.69807856e+11,\n",
       "         -1.67262126e+11,  1.09632835e+12,  2.21569090e+11,  8.12030296e+11,\n",
       "          3.18225252e+11,  1.10676043e+12,  4.76913926e+11,  7.13472737e+11,\n",
       "          2.15733961e+11,  6.11675603e+11,  2.26895888e+11,  4.70745973e+11,\n",
       "         -4.20226990e+10,  6.12289610e+11,  1.04467775e+11,  7.47477467e+11,\n",
       "          1.45114874e+11,  3.41610332e+11, -8.96882934e+10,  5.67414555e+11,\n",
       "         -1.03331766e+11,  3.62539876e+11, -2.43558056e+11,  1.48503806e+11,\n",
       "         -3.54567291e+11, -1.06713457e+10, -5.05658049e+11, -2.35127259e+11,\n",
       "         -6.51389567e+11, -4.13350068e+11, -7.54553389e+11, -6.41417740e+11,\n",
       "         -1.00093139e+12, -8.58808812e+10,  8.42754458e+09, -1.00873503e+12,\n",
       "          2.77221573e+11, -1.02296237e+12,  1.29445147e+12, -2.72123150e+11,\n",
       "          6.70627267e+11,  4.34893029e+11,  4.28608455e+11,  2.80795808e+11,\n",
       "          4.35126174e+11,  1.85318670e+11,  1.38223026e+11,  2.36546916e+11,\n",
       "          1.37910682e+11,  3.24281303e+11,  2.49202491e+11,  4.23410500e+11,\n",
       "          3.05474077e+11,  6.42632122e+11, -1.00183507e+11,  5.34143877e+10,\n",
       "          3.84239010e+11,  7.93810305e+11, -6.36304343e+10,  7.54804163e+10,\n",
       "          3.17884465e+11,  4.85184111e+11,  5.01512307e+11,  1.20692566e+12,\n",
       "         -2.43679658e+11,  4.08835031e+10,  4.63209693e+11,  1.41909112e+12,\n",
       "         -4.40061166e+11, -9.99146127e+10,  3.86516124e+11,  1.56515382e+12,\n",
       "         -1.33944525e+11, -7.61226199e+10, -7.57882798e+10,  9.44405299e+10],\n",
       "        dtype=float32),\n",
       "  array([0, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1])),\n",
       " (array([ 4.6870017e+11,  1.1754612e+12,  1.1970410e+12,  3.6270657e+11,\n",
       "          1.5695367e+11,  4.9753506e+10,  2.7106607e+12,  9.7606736e+11,\n",
       "          3.5854368e+12,  1.2001838e+12,  7.1877637e+11,  3.0810033e+11,\n",
       "          3.5851634e+12,  1.4195377e+12,  8.0815732e+11,  1.4751806e+11,\n",
       "          3.4621306e+12,  1.2506374e+12,  4.0606260e+11,  7.0569624e+10,\n",
       "          2.9886765e+12,  1.0996023e+12, -2.0908078e+11,  3.4331129e+11,\n",
       "         -3.3300310e+11,  2.2459468e+12,  3.6229530e+11,  1.6535553e+12,\n",
       "          6.7544305e+11,  2.2822338e+12,  9.7186008e+11,  1.5008419e+12,\n",
       "          4.8701846e+11,  1.2754230e+12,  5.6643545e+11,  1.1347662e+12,\n",
       "          9.5667765e+10,  1.4270765e+12,  3.8343246e+11,  1.7668477e+12,\n",
       "          5.2491908e+11,  9.2238512e+11, -4.9685238e+10,  1.1638203e+12,\n",
       "         -2.3546289e+11,  6.9936113e+11, -5.6178691e+11,  2.7808675e+11,\n",
       "         -8.3458366e+11, -1.2653416e+11, -9.5017494e+11, -4.1965892e+11,\n",
       "         -1.2336031e+12, -8.0053640e+11, -1.6943326e+12, -1.4394663e+12,\n",
       "         -1.9910322e+12, -1.7227224e+12, -2.1929842e+12, -9.1397089e+11,\n",
       "          8.3731585e+11,  5.1646418e+11,  2.8289542e+11, -1.8691888e+12,\n",
       "          7.7887668e+10, -3.3255716e+12,  2.9038957e+12, -1.3319007e+12,\n",
       "          8.6121814e+11,  2.6577733e+11,  6.5884088e+11,  5.2325030e+11,\n",
       "          3.8475648e+11,  6.2793830e+11,  6.9760936e+11,  9.7179252e+11,\n",
       "          7.2778272e+11,  1.3317016e+12,  1.2146961e+12,  2.0020080e+12,\n",
       "          4.1768305e+10,  4.2598515e+11,  2.6489330e+11,  4.0966835e+11,\n",
       "          3.3635117e+11,  4.6830905e+11,  4.1087631e+11,  4.9334275e+11,\n",
       "          5.3819015e+11,  5.3791942e+11,  4.4709583e+11,  4.0684723e+11,\n",
       "         -3.1870543e+11,  4.3264947e+11, -6.8660580e+11,  1.3847985e+11,\n",
       "         -9.5008050e+11,  2.8304838e+11, -8.0393286e+11,  3.2040659e+11],\n",
       "        dtype=float32),\n",
       "  array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2]))]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demonstrate recreation\n",
    "yhat = model.predict(scenes[:10], verbose=2)\n",
    "\n",
    "list(zip(yhat.reshape(10, Quantifier.scenes_len), \n",
    "         scenes[:10].reshape(10, Quantifier.scenes_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
